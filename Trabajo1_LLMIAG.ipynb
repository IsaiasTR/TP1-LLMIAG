{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "TNIZqJpxXquL"
      },
      "outputs": [],
      "source": [
        "PINECONE = {\n",
        "    \"API_KEY\": \"pcsk_2b78HU_TqDQRdqmBRhgvc6RFtLamaqMW6NTqq7kziXPLcZEa4aabk5iTSwraqVCx3dF6iy\",\n",
        "    \"ENVIRONMENT\": \"us-west1-gcp\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client sentence-transformers transformers odfpy gradio\n",
        "!pip install \"pinecone-client[grpc]\""
      ],
      "metadata": {
        "id": "VzdtMWaTYa_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install odfpy"
      ],
      "metadata": {
        "id": "AK4Zw4WzwuQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p docs_odt  # Crear carpeta si no existe\n",
        "!mv isaias_cv.odt  octavio_cv.odt santiago_cv.odt docs_odt/"
      ],
      "metadata": {
        "id": "5vHqH7-Mwwb2"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import odf.opendocument\n",
        "import odf.text\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "import random\n",
        "import gradio as gr\n",
        "import json"
      ],
      "metadata": {
        "id": "5deQjQmNYpBF"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración de Pinecone (reemplaza con tus credenciales)\n",
        "PINECONE = {\n",
        "    \"API_KEY\": \"pcsk_2b78HU_TqDQRdqmBRhgvc6RFtLamaqMW6NTqq7kziXPLcZEa4aabk5iTSwraqVCx3dF6iy\",  # Usa tu propia API key de Pinecone\n",
        "    \"ENVIRONMENT\": \"us-west1-gcp\"  # Cambia según tu región\n",
        "}\n",
        "\n",
        "# Inicializar el cliente de Pinecone\n",
        "pinecone_client = Pinecone(api_key=PINECONE[\"API_KEY\"], environment=PINECONE[\"ENVIRONMENT\"])\n",
        "\n",
        "# Crear o acceder al índice en Pinecone\n",
        "index_name = \"rag-index\"\n",
        "embedding_dimension = 384  # Asegúrate de que sea compatible con tu modelo de embeddings\n",
        "\n",
        "# Verificar si el índice existe, si no, lo crea\n",
        "if index_name not in pinecone_client.list_indexes().names():\n",
        "    pinecone_client.create_index(\n",
        "        name=index_name,\n",
        "        dimension=embedding_dimension,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n"
      ],
      "metadata": {
        "id": "KXXNR8_kYyOS"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar y leer el contenido del archivo .odt\n",
        "def leer_documentos_odt(ruta_archivo):\n",
        "    documento = odf.opendocument.load(ruta_archivo)\n",
        "    contenido = []\n",
        "    for elemento in documento.getElementsByType(odf.text.P):\n",
        "        # Extraer el texto de cada elemento <P> de forma segura\n",
        "        # Cambiado de odf.text.Text a odf.element.Text\n",
        "        texto_parrafo = ''.join([node.data for node in elemento.childNodes if isinstance(node, odf.element.Text)])\n",
        "        if texto_parrafo.strip():  # Asegurarse de que el texto no esté vacío\n",
        "            contenido.append(texto_parrafo.strip())\n",
        "    return \"\\n\".join(contenido)\n",
        "\n",
        "# Ruta del directorio que contiene los archivos .odt\n",
        "ruta_directorio = \"/content/docs_odt\"\n",
        "\n",
        "# Iterar sobre los archivos en el directorio\n",
        "for nombre_archivo in os.listdir(ruta_directorio):\n",
        "    # Verificar si el archivo es un archivo .odt\n",
        "    if nombre_archivo.endswith(\".odt\"):\n",
        "        # Construir la ruta completa del archivo\n",
        "        ruta_archivo = os.path.join(ruta_directorio, nombre_archivo)\n",
        "\n",
        "        # Leer el archivo y mostrar los primeros 100 caracteres\n",
        "        contenido_documento = leer_documentos_odt(ruta_archivo)\n",
        "        print(f\"Contenido del documento {nombre_archivo}: {contenido_documento[:100]}\")"
      ],
      "metadata": {
        "id": "DUbuI0GsY9jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalización de vectores usando Z-score\n",
        "def normalizar_vector_zscore(vector):\n",
        "    \"\"\"Normaliza un vector utilizando Z-score.\"\"\"\n",
        "    vector = np.array(vector)  # Convertir a NumPy array\n",
        "    mean = np.mean(vector)\n",
        "    std = np.std(vector)\n",
        "    if std == 0:  # Manejar el caso de desviación estándar 0\n",
        "        return vector\n",
        "    else:\n",
        "        return (vector - mean) / std"
      ],
      "metadata": {
        "id": "8qFZChJIDjMe"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo de embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generar embedding del contenido del documento\n",
        "embedding = model.encode(contenido_documento)\n",
        "\n",
        "print(f\"Dimensión del embedding: {len(embedding)}\")\n",
        "\n",
        "# Iterar sobre los archivos en el directorio\n",
        "for nombre_archivo in os.listdir(ruta_directorio):\n",
        "    if nombre_archivo.endswith(\".odt\"):\n",
        "        ruta_archivo = os.path.join(ruta_directorio, nombre_archivo)\n",
        "\n",
        "        # Leer el archivo\n",
        "        contenido_documento = leer_documentos_odt(ruta_archivo)\n",
        "\n",
        "        # Generar embedding del contenido del documento\n",
        "        embedding = model.encode(contenido_documento)\n",
        "        # ===>>> Add Normalization here <<<===\n",
        "        embedding = normalizar_vector_zscore(embedding).tolist()\n",
        "        # ===>>> End Normalization <<<===\n",
        "\n",
        "        # Crear un diccionario con el embedding y los metadatos\n",
        "        documento_id = nombre_archivo  # Usar el nombre del archivo como ID\n",
        "        metadata = {\"text\": contenido_documento}\n",
        "\n",
        "        # Subir el embedding a Pinecone\n",
        "        index = pinecone_client.Index(index_name)\n",
        "        index.upsert(vectors=[{\n",
        "            \"id\": documento_id,\n",
        "            \"values\": embedding,\n",
        "            \"metadata\": metadata\n",
        "        }])\n",
        "\n",
        "        print(f\"Documento '{documento_id}' subido a Pinecone.\")\n",
        "\n",
        "# Verificar estadísticas del índice\n",
        "stats = index.describe_index_stats()\n",
        "print(\"Estadísticas del índice:\", stats)"
      ],
      "metadata": {
        "id": "JRt9lBhBbCVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para consultar Pinecone y obtener documentos relevantes\n",
        "def consultar_pinecone(query, top_k=3):\n",
        "    # Generar el embedding de la consulta\n",
        "    query_embedding = model.encode(query).tolist()\n",
        "\n",
        "    # Normalizar el vector de consulta\n",
        "    query_embedding = normalizar_vector_zscore(query_embedding).tolist()\n",
        "\n",
        "    # Realizar la consulta en Pinecone\n",
        "    results = index.query(\n",
        "        vector=query_embedding,\n",
        "        top_k=top_k,\n",
        "        include_values=True,\n",
        "        include_metadata=True,\n",
        "        namespace=\"\"\n",
        "    )\n",
        "\n",
        "    return results.matches\n"
      ],
      "metadata": {
        "id": "FpQD5apbbOu4"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo de generación de texto\n",
        "#generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "# Función para generar la respuesta utilizando contexto recuperado\n",
        "def generar_respuesta(query, contexto):\n",
        "    if not contexto.strip():\n",
        "        return \"No se encontraron documentos relevantes para generar una respuesta.\"\n",
        "\n",
        "    prompt = f\"Contexto:\\n{contexto}\\n\\nPregunta: {query}\\nRespuesta:\"\n",
        "\n",
        "    try:\n",
        "        # Generar varias respuestas con temperatura y longitud máxima ajustadas\n",
        "        respuestas = generator(\n",
        "            prompt,\n",
        "            max_length=30,\n",
        "            num_beams=3,# Ajusta la longitud máxima según tus necesidades\n",
        "            num_return_sequences=3,  # Genera 3 respuestas diferentes\n",
        "            temperature=0.7  # Ajusta la temperatura para controlar la diversidad\n",
        "        )\n",
        "\n",
        "        # Seleccionar una respuesta aleatoria de las generadas\n",
        "        mejor_respuesta = random.choice(respuestas)['generated_text']\n",
        "        return mejor_respuesta\n",
        "    except Exception as e:\n",
        "        return f\"Error al generar la respuesta: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "N1F33JfEbSkb"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función principal del chatbot RAG\n",
        "def chatbot_rag(query):\n",
        "    try:\n",
        "        # Consultar Pinecone y obtener los resultados\n",
        "        resultados = consultar_pinecone(query, top_k=3)\n",
        "\n",
        "        if not resultados:\n",
        "            return \"No se encontraron documentos relevantes para esta consulta.\"\n",
        "\n",
        "        # Extraer el texto del contexto de los metadatos\n",
        "        contexto = \" \".join([res.metadata['text'] for res in resultados if res.metadata and res.metadata.get('text')])\n",
        "\n",
        "        # Verifica si se generó un contexto válido\n",
        "        if not contexto:\n",
        "            return \"No se pudieron construir contextos relevantes a partir de los documentos recuperados.\"\n",
        "\n",
        "        # Generar la respuesta usando la función generar_respuesta\n",
        "        respuesta = generar_respuesta(query, contexto)\n",
        "        return respuesta\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error en el flujo RAG: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "87usY4LwbZTW"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=chatbot_rag,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ingresa tu consulta aquí...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Chatbot RAG para CVs\",\n",
        "    description=\"Consulta información sobre  CVs cargados.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "Fk5xK65dBxsO",
        "outputId": "7fb7559a-e866-4524-cdfb-68b4c7e138f9"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://797bb5e4838980b94f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://797bb5e4838980b94f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    }
  ]
}